% Licensed to the Apache Software Foundation (ASF) under one or more
% contributor license agreements.  See the NOTICE file distributed with
% this work for additional information regarding copyright ownership.
% The ASF licenses this file to You under the Apache License, Version 2.0
% (the "License"); you may not use this file except in compliance with
% the License.  You may obtain a copy of the License at
%
%     http://www.apache.org/licenses/LICENSE-2.0
%
% Unless required by applicable law or agreed to in writing, software
% distributed under the License is distributed on an "AS IS" BASIS,
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
% See the License for the specific language governing permissions and
% limitations under the License.

\chapter{Replication}

\section{Overview}

Replication is a feature of Accumulo which provides a mechanism to automatically
copy data to other systems, typically for the purpose of disaster recovery,
high availability, or geographic locality. It is best to consider this feature
as a framework for automatic replication instead of the ability to copy data
from to another Accumulo instance as copying to another Accumulo cluster is
only an implementation detail. The local Accumulo cluster is hereby referred
to as the \texttt{primary} while systems being replicated to are known as
\texttt{peers}.

This replication framework makes two Accumulo instances, where one instance
replicates to another, eventually consistent between one another, as opposed
to the strong consistency that each single Accumulo instance still holds. That
is to say, attempts to read data from a table on a peer which has pending replication
from the primary will not wait for that data to be replicated before running the scan.
This is desirable for a number of reasons, the most important is that the replication
framework is not limited by network outages or offline peers, but only by the HDFS
space available on the primary system.

Replication configurations can be considered as a directed graph which allows cycles.
The systems in which data was replicated from is maintained in each Mutation which
allow each system to determine if a peer has already has the data in which
the system wants to send.

Data is replicated by using the Write-Ahead logs (WAL) that each TabletServer is
already maintaining. TabletServers records which WALs have data that need to be
replicated to the \texttt{accumulo.metadata} table. The Master uses these records,
combined with the local Accumulo table that the WAL was used with, to create records
in the \texttt{replication} table which track which peers the given WAL should be
replicated to. The Master latter uses these work entries to assign the actual
replication task to a local TabletServer using ZooKeeper. A TabletServer will get
a lock in ZooKeeper for the replication of this file to a peer, and proceed to
replicate to the peer, recording progress in the \textt{replication} table as
data is successfully replicated on the peer. Later, the Master and Garbage Collector
will remove records from the \textt{accumulo.metadata} and \textt{replication} tables
and files from HDFS, respectively, after replication to all peers is complete.

\subsection{Configuration}

Configuration of Accumulo to replicate data to another system can be categorized
into the following sections.

\subsection{Site Configuration}

Each system involved in replication (even the primary) needs a name that uniquely
identifies it across all peers in the replication graph. This should be considered
fixed for an instance, and set in \texttt{accumulo-site.xml}.

\begingroup\fontsize{8pt}{8pt}\selectfont\begin{verbatim}
<property>
    <name>replication.name</name>
    <value>primary</value>
    <description>Unique name for this system used by replication</description>
</property>
\end{verbatim}\endgroup

\section{Instance Configuration}

For each peer of this system, Accumulo needs to know the name of that peer,
the class used to replicate data to that system and some configuration information
to connect to this remote peer. In the case of Accumulo, this additional data
is the Accumulo instance name and ZooKeeper quorum; however, this varies on the
replication implementation for the peer.

These can be set in the site configuration to ease deployments; however, as they may
change, it can be useful to set this information using the Accumulo shell.

To configure a peer with the name \textt{peer1} which is an Accumulo system with an instance name of
\textt{accumulo_peer}
and a ZooKeeper quorum of \texttt{10.0.0.1,10.0.2.1,10.0.3.1}, invoke the following
command in the shell.

\begingroup\fontsize{8pt}{8pt}\selectfont\begin{verbatim}
root@accumulo_primary> config -s
replication.peer.peer1=org.apache.accumulo.tserver.replication.AccumuloReplicaSystem,accumulo_peer,10.0.0.1,10.0.2.1,10.0.3.1
\end{verbatim}\endgroup

Since this is an Accumulo system, we also want to set a username and password
to use when authenticating with this peer. On our peer, we make a special user
which has permission to write to the tables we want to replicate data into, "replication"
with a password of "password". We then need to record this in the primary's configuration.

\begingroup\fontsize{8pt}{8pt}\selectfont\begin{verbatim}
root@accumulo_primary> config -s replication.peer.user.peer1=replication
root@accumulo_primary> config -s replication.peer.password.peer1=password
\end{verbatim}\endgroup

\subsection{Table Configuration}

Now, we presently have a peer defined, so we just need to configure which tables will
replicate to that peer. We also need to configure an identifier to determine where
this data will be replicated on the peer. Since we're replicating to another Accumulo
cluster, this is a table ID. In this example, we want to enable replication on
\texttt{my_table} and configure our peer \texttt{accumulo_peer} as a target, sending
the data to the table with an ID of \texttt{2} in \texttt{accumulo_peer}.

\begingroup\fontsize{8pt}{8pt}\selectfont\begin{verbatim}
root@accumulo_primary> config -t my_table -s table.replication=true
root@accumulo_primary> config -t my_table -s table.replication.target.acccumulo_peer=2
\end{verbatim}\endgroup

To replicate a single table on the primary to multiple peers, the second command
in the above shell snippet can be issued, for each peer and remote identifier pair.

\section{Monitoring}

Basic information about replication status from a primary can be found on the Accumulo
Monitor server, using the \texttt{Replication} link the sidebar.

On this page, information is broken down into the following sections:

\begin{itemize}
    \item Files pending replication by peer and target
    \item Files queued for replication, with progress made
\end{itemize}

\section{Work Assignment}

Depending on the schema of a table, different implementations of the WorkAssigner used could
be configured. The implementation is controlled via the property \textt{replication.work.assigner}
and the full class name for the implementation. This can be configured via the shell or
\textt{accumulo-site.xml}.

\begingroup\fontsize{8pt}{8pt}\selectfont\begin{verbatim}
<property>
    <name>replication.work.assigner</name>
    <value>org.apache.accumulo.master.replication.SequentialWorkAssigner</value>
    <description>Implementation used to assign work for replication</description>
</property>
\end{verbatim}\endgroup

\begingroup\fontsize{8pt}{8pt}\selectfont\begin{verbatim}
root@accumulo_primary> config -t my_table -s replication.work.assigner=org.apache.accumulo.master.replication.SequentialWorkAssigner
\end{verbatim}\endgroup

Two implementations are provided. By default, the \textt{SequentialWorkAssigner} is configured for an
instance. The SequentialWorkAssigner ensures that, per peer and each remote identifier, each WAL is
replicated in the order in which they were created. This is sufficient to ensure that updates to a table
will be replayed in the correct order on the peer. This implementation has the downside of only replicating
a single WAL at a time.

The second implementation, the \textt{UnorderedWorkAssigner} can be used to overcome the limitation
of only a single WAL being replicated to a target and peer at any time. Depending on the table schema,
it's possible that multiple versions of the same Key with different values are infrequent or nonexistent.
In this case, parallel replication to a peer and target is possible without any downsides. In the case
where this implementation is used were column updates are frequent, it is possible that there will be
an inconsistency between the primary and the peer.

\section{ReplicaSystems}

\texttt{ReplicaSystem} is the interface which allows abstraction of replication of data
to peers of various types. Presently, only an \texttt{AccumuloReplicaSystem} is provided
which will replicate data to another Accumulo instance. A \texttt{ReplicaSystem} implementation
is run inside of the TabletServer process, and can be configured as mentioned in the 
\texttt{Instance Configuration} section of this document. Theoretically, an implementation
of this interface could send data to other filesystems, databases, etc.

\subsection{ AccumuloReplicaSystem}

The \texttt{AccumuloReplicaSystem} uses Thrift to communicate with a peer Accumulo instance
and replicate the necessary data. The TabletServer running on the primary will communicate
with the Master on the peer to request the address of a TabletServer on the peer which
this TabletServer will use to replicate the data.

The TabletServer on the primary will then replicate data in batches of a configurable
size (\texttt{replication.max.unit.size}). The TabletServer on the peer will report how many
records were applied back to the primary, which will be used to record how many records
were successfully replicated. The TabletServer on the primary will continue to replicate
data in these batches until no more data can be read from the file.
<<<<<<< HEAD:docs/src/main/latex/accumulo_user_manual/chapters/replication.tex
=======

=== Other Configuration

There are a number of configuration values that can be used to control how
the implementation of various components operate.

\begingroup\fontsize{8pt}{8pt}\selectfont\begin{verbatim}
Property, Description, Default
replication.max.work.queue, Maximum number of files queued for replication at one time, 1000
replication.work.assignment.sleep, Time between invocations of the WorkAssigner, 30s
replication.worker.threads, Size of threadpool used to replicate data to peers, 4
replication.receipt.service.port, Thrift service port to listen for replication requests, can use '0' for a random port, 10002
replication.work.attempts, Number of attempts to replicate to a peer before aborting the attempt, 10
replication.receiver.min.threads, Minimum number of idle threads for handling incoming replication, 1
replication.receiver.threadcheck.time, Time between attempting adjustments of thread pool for incoming replications, 30s
replication.max.unit.size, Maximum amount of data to be replicated in one RPC, 64M
replication.work.assigner, Work Assigner implementation, org.apache.accumulo.master.replication.SequentialWorkAssigner
tserver.replication.batchwriter.replayer.memory,  Size of BatchWriter cache to use in applying replication requests, 50M
\end{verbatim}\endgroup

\section{Example Practical Configuration}

A real-life example is now provided to give concrete application of replication configuration. This
example is a two instance Accumulo system, one primary system and one peer system. They are called
primary and peer, respectively. Each system also have a table of the same name, "my_table". The instance
name for each is also the same (primary and peer), and both have ZooKeeper hosts on a node with a hostname
with that name as well (primary:2181 and peer:2181).

We want to configure these systems so that "my_table" on "primary" replicates to "my_table" on "peer".

\subsection{conf/accumulo-site.xml}

We can assign the "unique" name that identifies this Accumulo instance among all others that might participate
in replication together. In this example, we will use the names provided in the description.

\subsection{Primary}

\begingroup\fontsize{8pt}{8pt}\selectfont\begin{verbatim}
<property>
  <name>replication.name</name>
  <value>primary</value>
  <description>Defines the unique name</description>
</property>
\end{verbatim}\endgroup

\subsection{Peer}

\begingroup\fontsize{8pt}{8pt}\selectfont\begin{verbatim}
<property>
  <name>replication.name</name>
  <value>peer</value>
</property>
\end{verbatim}\endgroup

\subsection{conf/masters and conf/slaves}

Be *sure* to use non-local IP addresses. Other nodes need to connect to it and using localhost will likely result in
a local node talking to another local node.

\subsection{Start both instances}

The rest of the configuration is dynamic and is best configured on the fly (in ZooKeeper) than in accumulo-site.xml.

\subsection{Peer}

The next series of command are to be run on the peer system. Create a user account for the primary instance called
"peer". The password for this account will need to be saved in the configuration on the primary

\begingroup\fontsize{8pt}{8pt}\selectfont\begin{verbatim}
root@peer> createtable my_table
root@peer> createuser peer
root@peer> grant -t my_table -u peer Table.WRITE
root@peer> grant -t my_table -u peer Table.READ
root@peer> tables -l
\end{verbatim}\endgroup

Remember what the table ID for 'my_table' is. You'll need that to configured the primary instance.

\subsection{Primary}

Next, configure the primary instance.

\subsection{Set up the table}

\begingroup\fontsize{8pt}{8pt}\selectfont\begin{verbatim}
root@primary> createtable my_table
\end{verbatim}\endgroup

\subsection{Define the Peer as a replication peer to the Primary}

We're defining the instance with replication.name of 'peer' as a peer. We provide the implementation of ReplicaSystem
that we want to use, and the configuration for the AccumuloReplicaSystem. In this case, the configuration is the Accumulo
Instance name for 'peer' and the ZooKeeper quorum string. The configuration key is of the form
"replication.peer.$peer_name".

\begingroup\fontsize{8pt}{8pt}\selectfont\begin{verbatim}
root@primary> config -s replication.peer.peer=org.apache.accumulo.tserver.replication.AccumuloReplicaSystem,peer,$peer_zk_quorum
\end{verbatim}\endgroup

\subsection{Set the authentication credentials}

We want to use that special username and password that we created on the peer, so we have a means to write data to
the table that we want to replicate to. The configuration key is of the form "replication.peer.user.$peer_name".

\begingroup\fontsize{8pt}{8pt}\selectfont\begin{verbatim}
root@primary> config -s replication.peer.user.peer=peer
root@primary> config -s replication.peer.password.peer=peer
\end{verbatim}\endgroup

\subsection{Enable replication on the table}

Now that we have defined the peer on the primary and provided the authentication credentials, we need to configure
our table with the implementation of ReplicaSystem we want to use to replicate to the peer. In this case, our peer 
is an Accumulo instance, so we want to use the AccumuloReplicaSystem.

The configuration for the AccumuloReplicaSystem is the table ID for the table on the peer instance that we
want to replicate into. Be sure to use the correct value for $peer_table_id. The configuration key is of
the form "table.replication.target.$peer_name".

\begingroup\fontsize{8pt}{8pt}\selectfont\begin{verbatim}
root@primary> config -t my_table -s table.replication.target.peer=$peer_table_id
\end{verbatim}\endgroup

Finally, we can enable replication on this table.

\begingroup\fontsize{8pt}{8pt}\selectfont\begin{verbatim}
root@primary> config -t my_table -s table.replication=true
\end{verbatim}\endgroup
